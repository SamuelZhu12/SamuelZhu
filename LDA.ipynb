{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from gensim.models import doc2vec,ldamodel\n",
    "from gensim import corpora,similarities\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email_text(text):\n",
    "    # 数据清洗\n",
    "    text = text.replace('\\n', \" \")  # 新行，我们是不需要的\n",
    "    text = re.sub(r\"-\", \" \", text)  # 把 \"-\" 的两个单词，分开。（比如：july-edu ==> july edu）\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text)  # 日期，对主体模型没什么意义\n",
    "    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text)  # 时间，没意义\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text)  # 邮件地址，没意义\n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text)  # 网址，没意义\n",
    "    pure_text = ''\n",
    "    # 以防还有其他特殊字符（数字）等等，我们直接把他们loop一遍，过滤掉\n",
    "    for letter in text:\n",
    "        # 只留下字母和空格\n",
    "        if letter.isalpha() or letter == ' ':\n",
    "            pure_text += letter\n",
    "    # 再把那些去除特殊字符后落单的单词，直接排除。\n",
    "    # 我们就只剩下有意义的单词了。\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word) > 1)  # 而且单词长度必须是2以上\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords():\n",
    "    stopwords = []\n",
    "    with open('stopwords.txt','r',encoding='utf-8') as f:\n",
    "        stopwords = f.read().split()\n",
    "    return stopwords   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, 0.23996672), (16, 0.25371647), (18, 0.3645104)]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv('Emails.csv')\n",
    "    df = df[['Id','ExtractedBodyText']].dropna()\n",
    "\n",
    "    docs =df['ExtractedBodyText']\n",
    "    docs = docs.apply(lambda s:clean_email_text(s))\n",
    "    doclist = docs.values\n",
    "    \n",
    "    dellist = stopwords()\n",
    "    texts = [[word for word in doc.lower().split() if word not in dellist] for doc in doclist]\n",
    "    \n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    \n",
    "    lda = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20,random_state=None,passes=10)\n",
    "    \n",
    "    lda.save('samuellda.model')\n",
    "    \n",
    "    lda = ldamodel.LdaModel.load('samuellda.model')\n",
    "    text1 = 'I was greeted by this heartwarming display on the corner of my street today. ' \\\n",
    "           'Thank you to all of you who did this. Happy Thanksgiving. -H'\n",
    "    text1 = clean_email_text(text1)\n",
    "    texts1 = [word for word in text1.lower().split() if word not in dellist]\n",
    "    bow1 = dictionary.doc2bow(texts1)\n",
    "    doc_lda1 = lda[bow1]\n",
    "    list_doc1 = [i[1] for i in doc_lda1]\n",
    "\n",
    "#     print(lda.get_document_topics(bow1))\n",
    "#     doc_lda1 = lda[bow1]\n",
    "#     print(doc_lda1)\n",
    "\n",
    "    text2 = 'I always love to greet with my friends at the corner of the street.'\n",
    "    text2 = clean_email_text(text2)\n",
    "    texts2 = [word for word in text2.lower().split() if word not in dellist]\n",
    "    bow2 = dictionary.doc2bow(texts2)\n",
    "    doc_lda2 = lda[bow2]\n",
    "    list_doc2 = [i[1] for i in doc_lda2]\n",
    "    \n",
    "    try:\n",
    "        sim = np.dot(list_doc1, list_doc2) / (np.linalg.norm(list_doc1) * np.linalg.norm(list_doc2))\n",
    "    except ValueError:\n",
    "        sim=0\n",
    "\n",
    "    print(doc_lda1)\n",
    "#     with open('Emails.csv','r',encoding='utf-8') as p:\n",
    "#         txt=p.read()\n",
    "#     print(txt.lower().split())\n",
    "#     txt = [word for word in txt.lower().split() if word not in dellist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972678343474334\n",
      "[1.2 0.2 0.3 0.4 0.5 0.6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "list = [1.2,0.2,0.3,0.4,0.5,0.6]\n",
    "list1 = [1,0.5,0.5,0.2,0.3,0.6]\n",
    "array = np.array(list)\n",
    "array1 = np.array(list1)\n",
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a \n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    return sim\n",
    "print(cos_sim(list,list1))\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-92c38d3c89bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mFear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mDisgust\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'情感分类'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'PA'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mHappy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'词语'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "##情绪词语列表整理\n",
    "Happy = []\n",
    "Good = []\n",
    "Surprise = []\n",
    "Anger = []\n",
    "Sad = []\n",
    "Fear = []\n",
    "Disgust = []\n",
    "for idx,row in df.iterrows():\n",
    "    if row['情感分类'] in ['PA','PE']:\n",
    "        Happy.append(row['词语'])\n",
    "    if row['情感分类'] in ['PD','PH','PG','PB','PK']:\n",
    "        Good.append(row['词语'])\n",
    "    if row['情感分类'] in ['PC']:\n",
    "        Surprise.append(row['词语'])\n",
    "    if row['情感分类'] in ['NA']:\n",
    "        Anger.append(row['词语'])\n",
    "    if row['情感分类'] in ['NB','NJ','NH','PF']:\n",
    "        Sad.append(row['词语'])\n",
    "    if row['情感分类'] in ['NI','NC','NG']:\n",
    "        Fear.append(row['词语'])\n",
    "    if row['情感分类'] in ['NE','ND','NN','NK','NL']:\n",
    "        Disgust.append(row['词语'])\n",
    "Positive = Happy+Good+Surprise\n",
    "Negative = Anger+Sad+Fear+Disgust\n",
    "print('well_done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
